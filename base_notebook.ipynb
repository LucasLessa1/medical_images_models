{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin Lesions Dataset Description\n",
    "\n",
    "The dataset comprises various types of skin lesions, each falling under specific categories:\n",
    "\n",
    "- **Actinic Keratoses and Intraepithelial Carcinoma / Bowen's Disease (AKIEC)**\n",
    "- **Basal Cell Carcinoma (BCC)**\n",
    "- **Benign Keratosis-like Lesions**:\n",
    "  - *Solar Lentigines*\n",
    "  - *Seborrheic Keratoses*\n",
    "  - *Lichen-Planus like Keratoses (BKL)*\n",
    "- **Dermatofibroma (DF)**\n",
    "- **Melanoma (MEL)**\n",
    "- **Melanocytic Nevi (NV)**\n",
    "- **Vascular Lesions**:\n",
    "  - *Angiomas*\n",
    "  - *Angiokeratomas*\n",
    "  - *Pyogenic Granulomas*\n",
    "  - *Hemorrhage (VASC)*\n",
    "\n",
    "**Diagnosis Confirmation:**\n",
    "\n",
    "- Over 50% of lesions in this dataset are confirmed through **histopathology (Histo)**, serving as the ground truth for these cases.\n",
    "- For the remaining cases, confirmation methods include:\n",
    "  - *Follow-up examination (Follow_Up)*\n",
    "  - *Expert Consensus (Consensus)*\n",
    "  - *Confirmation by In-Vivo Confocal Microscopy (Confocal)*\n",
    "\n",
    "Each lesion might have multiple associated images, allowing for tracking via the `lesion_id` column within the `HAM10000_metadata` file.\n",
    "\n",
    "This diverse dataset contains various types of skin lesions, each categorized and confirmed through different diagnostic approaches, contributing to a comprehensive resource for research and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_dict = {\n",
    "#     'akiec': \"Actinic Keratoses and Intraepithelial Carcinoma / Bowen's Disease (AKIEC)\",\n",
    "#     'bcc': \"Basal Cell Carcinoma (BCC)\",\n",
    "#     'bkl': \"Benign Keratosis-like Lesions\",\n",
    "#     'df': \"Dermatofibroma (DF)\",\n",
    "#     'mel': \"Melanoma (MEL)\",\n",
    "#     'nv': \"Melanocytic Nevi (NV)\",\n",
    "#     'vasc': \"Vascular Lesions\",\n",
    "#     'histo': \"Confirmed through Histopathology (Histo)\",\n",
    "#     'follow_up': \"Follow-up examination (Follow_Up)\",\n",
    "#     'consensus': \"Expert Consensus (Consensus)\",\n",
    "#     'confocal': \"Confirmation by In-Vivo Confocal Microscopy (Confocal)\"\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import glob\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "import PIL \n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from tool_preprocessing import *\n",
    "# from models import *\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision.models as models\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "# from torchinfo import summary \n",
    "\n",
    "# import torch.optim as optim\n",
    "# from IPython.display import Image\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# from torchvision.datasets import ImageFolder\n",
    "# from torchvision.transforms import transforms\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import pydicom\n",
    "# from PIL import Image\n",
    "\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, labels are initially considered as categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the images are organized in the folders of each label, the following flag must be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_folder_sep = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = 'C:/Users/lucas/OneDrive - unb.br/Documents/UnB/Semestres-ENE/TCC/COVID_Dataset_original'\n",
    "base_path = 'C:/Users/lucas/OneDrive - unb.br/Documents/UnB/Semestres-ENE/TCC/The HAM10000 dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_folder_sep :\n",
    "    label_column = 'label'\n",
    "    train_df, test_df, val_df  = make_dataset_by_folder(base_path=base_path, label_column=label_column)\n",
    "\n",
    "else:\n",
    "    \n",
    "    path_train_df = f'{base_path}/HAM10000_metadata'\n",
    "    path_test_df = f'{base_path}/test.csv'\n",
    "    \n",
    "    path_train = f\"{base_path}/treino\"\n",
    "    path_test = f\"{base_path}/test\"\n",
    "    \n",
    "    paths_image = [path_train, path_test]\n",
    "    paths_df = [path_train_df, path_test_df]\n",
    "    label_column = 'dx'\n",
    "    \n",
    "    train_df, test_df, val_df = make_dataset_by_df(paths_image, paths_df, label_column=label_column)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_analysis_train = image_analysis(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smallest pixel value: 0\n",
    "\n",
    "Largest pixel value: 255\n",
    "\n",
    "Total images processed: 10015\n",
    "\n",
    "Channel Statistics:\n",
    "Channel 'R':\n",
    "  - Average: 194.6979202056175\n",
    "  - Standard Deviation: 22.85509458222392\n",
    "Channel 'G':\n",
    "  - Average: 139.26262746509866\n",
    "  - Standard Deviation: 30.1684115555478\n",
    "Channel 'B':\n",
    "  - Average: 145.4852413568536\n",
    "  - Standard Deviation: 33.90319049131724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = check_images_existence(train_df, path_column='path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 9010\n",
      "Number of unique labels: 7\n",
      "Label 'nv' has 6034 images.\n",
      "Label 'mel' has 1001 images.\n",
      "Label 'bkl' has 989 images.\n",
      "Label 'bcc' has 462 images.\n",
      "Label 'akiec' has 294 images.\n",
      "Label 'vasc' has 127 images.\n",
      "Label 'df' has 103 images.\n"
     ]
    }
   ],
   "source": [
    "dict_train_qntd = get_label_counts_and_print(train_df, label_column=label_column)\n",
    "# shapes_train = analyze_image_shapes(train_df, min_shape=(800, 800), path_column='path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nv': 6034,\n",
       " 'mel': 1001,\n",
       " 'bkl': 989,\n",
       " 'bcc': 462,\n",
       " 'akiec': 294,\n",
       " 'vasc': 127,\n",
       " 'df': 103}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_train_qntd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_analysis_test = image_analysis(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smallest pixel value: 0\n",
    "\n",
    "Largest pixel value: 255\n",
    "\n",
    "Total images processed: 1511\n",
    "\n",
    "Channel Statistics:\n",
    "Channel 'R':\n",
    "  - Average: 193.96235187636344\n",
    "  - Standard Deviation: 24.606448726550262\n",
    "Channel 'G':\n",
    "  - Average: 141.7550379243572\n",
    "  - Standard Deviation: 31.9625774054364\n",
    "Channel 'B':\n",
    "  - Average: 147.87214814814814\n",
    "  - Standard Deviation: 35.78649271231254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image not found in folder: None\n",
      "Removed lines:\n",
      "lesion_id       HAMTEST_0000496\n",
      "image_id           ISIC_0035068\n",
      "dx                           nv\n",
      "dx_type               consensus\n",
      "age                         NaN\n",
      "sex                         NaN\n",
      "localization                NaN\n",
      "dataset                     NaN\n",
      "path                       None\n",
      "Name: 534, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_df = check_images_existence(test_df, path_column='path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 1511\n",
      "Number of unique labels: 7\n",
      "Label 'nv' has 908 images.\n",
      "Label 'bkl' has 217 images.\n",
      "Label 'mel' has 171 images.\n",
      "Label 'bcc' has 93 images.\n",
      "Label 'df' has 44 images.\n",
      "Label 'akiec' has 43 images.\n",
      "Label 'vasc' has 35 images.\n"
     ]
    }
   ],
   "source": [
    "dict_test_qntd = get_label_counts_and_print(test_df, label_column=label_column)\n",
    "# shapes_test = analyze_image_shapes(test_df, min_shape=(461, 601), path_column='path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest pixel value: 0\n",
      "Largest pixel value: 255\n",
      "Total images processed: 1005\n",
      "Channel Statistics:\n",
      "Channel 'R':\n",
      "  - Average: 194.84180818500093\n",
      "  - Standard Deviation: 23.428856516708883\n",
      "Channel 'G':\n",
      "  - Average: 139.9250088557214\n",
      "  - Standard Deviation: 30.539483110296192\n",
      "Channel 'B':\n",
      "  - Average: 146.21765087525336\n",
      "  - Standard Deviation: 34.290640143609394\n"
     ]
    }
   ],
   "source": [
    "image_analysis_val = image_analysis(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = check_images_existence(val_df, path_column='path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 1005\n",
      "Number of unique labels: 7\n",
      "Label 'nv' has 671 images.\n",
      "Label 'mel' has 112 images.\n",
      "Label 'bkl' has 110 images.\n",
      "Label 'bcc' has 52 images.\n",
      "Label 'akiec' has 33 images.\n",
      "Label 'vasc' has 15 images.\n",
      "Label 'df' has 12 images.\n"
     ]
    }
   ],
   "source": [
    "dict_val_qntd = get_label_counts_and_print(val_df, label_column=label_column)\n",
    "# shapes_val = analyze_image_shapes(val_df, min_shape=(461, 601), path_column='path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average image shape - Height: 450.0, Width: 600.0\n",
    "\n",
    "Number of images with shape smaller than (461, 601): 1005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retirar depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_val = {}\n",
    "shapes_val['average_height'] = 450\n",
    "shapes_val['average_width'] = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passar de categorial para bin√°rio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pesos para a loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorial to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nv': 0, 'mel': 1, 'bkl': 2, 'bcc': 3, 'akiec': 4, 'vasc': 5, 'df': 6}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict = labels2dict(train_df, label_column)\n",
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label, test_label, val_label = dflabel2number([train_df, test_df, val_df], labels_dict, label_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21331502438562433, 1.2858570001427145, 1.3014589050989456, 2.7860235003092146, 4.3780369290573375, 10.13498312710911, 12.496532593619973]\n"
     ]
    }
   ],
   "source": [
    "if len(labels_dict) == 1:\n",
    "    weights = calculate_weights(train_df, labels_dict, dict_train_qntd)\n",
    "    weights = max(weights)\n",
    "else:\n",
    "    weights = calculate_weights(train_df, labels_dict, dict_train_qntd)\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\anaconda3\\envs\\torch000\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CT_Dataset(Dataset):\n",
    "    def __init__(self, img_path, img_labels, img_transforms=None, grayscale=False):\n",
    "        self.img_path = img_path\n",
    "        self.img_labels = torch.Tensor(img_labels)\n",
    "        if grayscale == True:\n",
    "            self.transforms = transforms.Compose([transforms.Grayscale(),\n",
    "                                                #   transforms.Resize((250, 250)),\n",
    "                                                  transforms.ToTensor()])\n",
    "        elif grayscale == False:\n",
    "            self.transforms = transforms.Compose([#transforms.Resize((250, 250)),\n",
    "                                                  transforms.ToTensor()])\n",
    "        else:\n",
    "            self.transforms = img_transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        cur_path = self.img_path[index]\n",
    "        cur_img = PIL.Image.open(cur_path).convert('RGB')\n",
    "        cur_img = self.transforms(cur_img)\n",
    "\n",
    "        return cur_img, self.img_labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CT_Dataset(img_path=np.array(train_df['path']), img_labels=np.array(train_label))\n",
    "val_dataset = CT_Dataset(img_path=np.array(val_df['path']), img_labels=np.array(val_label))\n",
    "test_dataset = CT_Dataset(img_path=np.array(test_df['path']), img_labels=np.array(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 0.0 MB\n",
      "Max GPU memory usage: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Current GPU memory usage:\", torch.cuda.memory_allocated() / (1024 ** 2), \"MB\")\n",
    "print(\"Max GPU memory usage:\", torch.cuda.max_memory_allocated() / (1024 ** 2), \"MB\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 124\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_R = image_analysis_val['channel_statistics']['R']['average']\n",
    "mean_G = image_analysis_val['channel_statistics']['G']['average']\n",
    "mean_B = image_analysis_val['channel_statistics']['B']['average']\n",
    "\n",
    "channels = 1 if mean_R == mean_G == mean_B else 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas\\anaconda3\\envs\\torch000\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start:\n",
      "Epoch:1 / 50, lr: 0.00010 train loss:1.51208, train acc: 0.51662, valid loss:1.01806, valid acc:0.59961\n",
      "The best val loss is 1.0180602073669434.\n",
      "\n",
      "Epoch:2 / 50, lr: 0.00010 train loss:1.16839, train acc: 0.59076, valid loss:1.26607, valid acc:0.49219\n",
      "The best val loss is 1.0180602073669434.\n",
      "\n",
      "Epoch:3 / 50, lr: 0.00010 train loss:1.06680, train acc: 0.62544, valid loss:1.08100, valid acc:0.65137\n",
      "The best val loss is 1.0180602073669434.\n",
      "\n",
      "Epoch:4 / 50, lr: 0.00010 train loss:0.93196, train acc: 0.64262, valid loss:0.78613, valid acc:0.69727\n",
      "The best val loss is 0.7861342430114746.\n",
      "\n",
      "Epoch:5 / 50, lr: 0.00010 train loss:0.86900, train acc: 0.64750, valid loss:0.90410, valid acc:0.62891\n",
      "The best val loss is 0.7861342430114746.\n",
      "\n",
      "Epoch:6 / 50, lr: 0.00010 train loss:0.75607, train acc: 0.68739, valid loss:0.82100, valid acc:0.67773\n",
      "The best val loss is 0.7861342430114746.\n",
      "\n",
      "Epoch:7 / 50, lr: 0.00005 train loss:0.63436, train acc: 0.71243, valid loss:0.86992, valid acc:0.69043\n",
      "The best val loss is 0.7861342430114746.\n",
      "\n",
      "Epoch:8 / 50, lr: 0.00005 train loss:0.43819, train acc: 0.77283, valid loss:0.80680, valid acc:0.71680\n",
      "The best val loss is 0.7861342430114746.\n",
      "\n",
      "Epoch:9 / 50, lr: 0.00005 train loss:0.30677, train acc: 0.81139, valid loss:0.55988, valid acc:0.78613\n",
      "The best val loss is 0.5598766803741455.\n",
      "\n",
      "Epoch:10 / 50, lr: 0.00005 train loss:0.24861, train acc: 0.83865, valid loss:0.65388, valid acc:0.75391\n",
      "The best val loss is 0.5598766803741455.\n",
      "\n",
      "Epoch:11 / 50, lr: 0.00005 train loss:0.17267, train acc: 0.87511, valid loss:0.65424, valid acc:0.77344\n",
      "The best val loss is 0.5598766803741455.\n",
      "\n",
      "Epoch:12 / 50, lr: 0.00005 train loss:0.19650, train acc: 0.87035, valid loss:0.63876, valid acc:0.76074\n",
      "The best val loss is 0.5598766803741455.\n",
      "\n",
      "Epoch:13 / 50, lr: 0.00005 train loss:0.12138, train acc: 0.91578, valid loss:0.66343, valid acc:0.75391\n",
      "The best val loss is 0.5598766803741455.\n",
      "\n",
      "Epoch:14 / 50, lr: 0.00003 train loss:0.09808, train acc: 0.92531, valid loss:0.57869, valid acc:0.81738\n",
      "The best val loss is 0.5598766803741455.\n",
      "\n",
      "Epoch:15 / 50, lr: 0.00003 train loss:0.04067, train acc: 0.96476, valid loss:0.56714, valid acc:0.80859\n",
      "The best val loss is 0.5598766803741455.\n",
      "\n",
      "Epoch:16 / 50, lr: 0.00003 train loss:0.03041, train acc: 0.97617, valid loss:0.50581, valid acc:0.84180\n",
      "The best val loss is 0.5058103799819946.\n",
      "\n",
      "Epoch:17 / 50, lr: 0.00003 train loss:0.02254, train acc: 0.98216, valid loss:0.54187, valid acc:0.83301\n",
      "The best val loss is 0.5058103799819946.\n",
      "\n",
      "Epoch:18 / 50, lr: 0.00003 train loss:0.01798, train acc: 0.98648, valid loss:0.61760, valid acc:0.80859\n",
      "The best val loss is 0.5058103799819946.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epoch = 50\n",
    "\n",
    "\n",
    "\n",
    "model_kernel = ResNet50(num_classes=7, input_channels=channels)\n",
    "# model_kernel = ResNet101(num_classes=7, input_channels=channels)\n",
    "# model_kernel = EfficientNetB(num_classes=7, input_channels=channels)\n",
    "# model_kernel = EfficientNetB4(num_classes=7, input_channels=channels)\n",
    "# model_kernel = EfficientNetB7(num_classes=7, input_channels=channels)\n",
    "# model_kernel = VGG16(num_classes=7, input_channels=channels)\n",
    "\n",
    "\n",
    "\n",
    "path_save_model = f'C:/Users/Lucas/Documents/PIBIC/DATASET/NIH-CHEST/model_/{model_kernel.get_name()}_{epoch}'\n",
    "\n",
    "trainer = ModelTrainer(model_kernel, device, weights, train_dataset, val_dataset, test_dataset, batch_size= batch_size, epochs=epoch)\n",
    "history, model, metrics = trainer.training()\n",
    "\n",
    "# hist_kernel, model_kernel, acc = train_model(model_kernel, train_dataset, val_dataset, test_dataset, device, path_save_model,weights, batch_size= batch_size, epochs=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Images - True Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_pred_true(model_kernel, test_dataset, device, num_images_to_plot=10, plot_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_kernel.state_dict(), f'Model_{model_kernel.get_name()}__Epoch_{epoch}__Batch_{batch_size}__Accuracy_{trainer.metrics_df['Accuracy'].loc[0]}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
